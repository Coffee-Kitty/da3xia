{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 定义n子棋游戏"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import import_ipynb\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import copy\n",
    "from collections import deque, defaultdict\n",
    "import random\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "class Configuration:\n",
    "\n",
    "    def __init__(self):\n",
    "        # 游戏\n",
    "        self.width = 5\n",
    "        self.height = 7\n",
    "        self.n_in_row = 4  # n子棋\n",
    "        self.Black = 1\n",
    "        self.White = -1\n",
    "        self.Continue = 0\n",
    "        self.Blackstone = 1\n",
    "        self.Whitestone = -1\n",
    "        self.Empty = 0\n",
    "\n",
    "        # train\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        self.learn_rate = 2e-3 # 基准学习率\n",
    "        self.lr_multiplier = 1.0  # 基于KL自动调整学习倍速\n",
    "        self.temp = 1.0  # 温度参数\n",
    "        self.search_time = 3  # 每下一步棋，搜索时间\n",
    "        self.c_puct = 5 # exploitation和exploration之间的折中系数\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 512  # mini-batch size for training\n",
    "        self.data_buffer = deque(maxlen=self.buffer_size) #使用 deque 创建一个双端队列\n",
    "        self.play_batch_size = 1\n",
    "        self.epochs = 5  # num of train_steps for each update\n",
    "        self.kl_targ = 0.02 # 早停检查\n",
    "        self.check_freq = 50 # 每50次检查一次，策略价值网络是否更新\n",
    "        self.game_batch_epoch= 500 # 训练多少个epoch\n",
    "        self.best_win_ratio = 0.0 # 当前最佳胜率，用他来判断是否有更好的模型\n",
    "        # 弱AI（纯MCTS）模拟步数，用于给训练的策略AI提供对手\n",
    "        self.pure_mcts_playout_num = 1000\n",
    "        self.init_model = \"best_policy.model\"\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,config = Configuration()):\n",
    "        self.width = config.width\n",
    "        self.height = config.height\n",
    "        self.n_in_row = config.n_in_row\n",
    "        self.Black = config.Black\n",
    "        self.White = config.White\n",
    "        self.Blackstone = config.Blackstone\n",
    "        self.Whitestone = config.Whitestone\n",
    "        self.Empty = config.Empty\n",
    "        self.Continue = config.Continue\n",
    "        self.cur_player = self.Black\n",
    "        self.board = [self.Empty for i in range(self.width*self.height)]\n",
    "        self.history = []\n",
    "\n",
    "    def xy2move(self,x, y):\n",
    "        return x * self.height + y\n",
    "\n",
    "    def move2xy(self, move):\n",
    "        return move // self.height, move % self.height\n",
    "\n",
    "    def play(self, move):\n",
    "        assert self.board[move] == self.Empty, f\"{move} has stone error -> play\"\n",
    "        if self.cur_player == self.Black:\n",
    "            self.board[move] = self.Blackstone\n",
    "        else:\n",
    "            self.board[move] = self.Whitestone\n",
    "        self.cur_player = -self.cur_player\n",
    "        self.history.append(move)\n",
    "\n",
    "        flag, winner = self.winner()\n",
    "        if flag:\n",
    "            return winner\n",
    "        else:\n",
    "            return self.Continue\n",
    "\n",
    "    def unplay(self, move):\n",
    "        assert self.board[move] != self.Empty, f\"{move} has not stone error -> unplay\"\n",
    "        self.cur_player = -self.cur_player\n",
    "        self.board[move] = self.Empty\n",
    "        self.history.pop()\n",
    "\n",
    "    def visual(self):\n",
    "        '''\n",
    "        (0, 6)(1, 6)(2, 6)(3, 6)(4, 6)\n",
    "        (0, 5)(1, 5)(2, 5)(3, 5)(4, 5)\n",
    "        (0, 4)(1, 4)(2, 4)(3, 4)(4, 4)\n",
    "        (0, 3)(1, 3)(2, 3)(3, 3)(4, 3)\n",
    "        (0, 2)(1, 2)(2, 2)(3, 2)(4, 2)\n",
    "        (0, 1)(1, 1)(2, 1)(3, 1)(4, 1)\n",
    "        (0, 0)(1, 0)(2, 0)(3, 0)(4, 0)\n",
    "\n",
    "\n",
    "        6 13 20 27 34\n",
    "        5 12 19 26 33\n",
    "        4 11 18 25 32\n",
    "        3 10 17 24 31\n",
    "        2 9 16 23 30\n",
    "        1 8 15 22 29\n",
    "        0 7 14 21 28\n",
    "\n",
    "        '''\n",
    "        for y in reversed(range(self.height)):\n",
    "            for x in range(self.width):\n",
    "                move = self.xy2move(x, y)\n",
    "                if self.board[move] == self.Blackstone:\n",
    "                    print('B\\t',end='')\n",
    "                elif self.board[move] == self.Whitestone:\n",
    "                    print('W\\t',end='')\n",
    "                    # print(f'{self.xy2move(x,y)}',end=' ')\n",
    "                else:\n",
    "                    print('-\\t', end='')\n",
    "                    # print(f'{self.xy2move(x,y)}',end=' ')\n",
    "            print()\n",
    "        print('*'*50)\n",
    "\n",
    "    def winner(self):\n",
    "        for i, m in enumerate(self.history):\n",
    "            player = self.Black if i % 2==0 else self.White\n",
    "            Stone = self.Blackstone if i%2 ==0 else self.Whitestone\n",
    "            x , y = self.move2xy(m)\n",
    "\n",
    "            # 水平\n",
    "            count = 0\n",
    "            for j in range(self.n_in_row):\n",
    "                if x+j >= self.width:\n",
    "                    count = 0\n",
    "                    break\n",
    "                if self.board[self.xy2move(x+j ,y)] == Stone:\n",
    "                    count += 1\n",
    "                    if count >= self.n_in_row:\n",
    "                        return True, player\n",
    "                else:\n",
    "                    count = 0\n",
    "                    break\n",
    "\n",
    "            # 竖直\n",
    "            for j in range(self.n_in_row):\n",
    "                if y+j >= self.height:\n",
    "                    count = 0\n",
    "                    break\n",
    "                if self.board[self.xy2move(x ,y+j)] == Stone:\n",
    "                    count += 1\n",
    "                    if count >= self.n_in_row:\n",
    "                        return True, player\n",
    "                else:\n",
    "                    count = 0\n",
    "                    break\n",
    "\n",
    "            # y=x\n",
    "            for j in range(self.n_in_row):\n",
    "                if y+j >= self.height or x+j >=self.width:\n",
    "                    count = 0\n",
    "                    break\n",
    "                if self.board[self.xy2move(x+j ,y+j)] == Stone:\n",
    "                    count += 1\n",
    "                    if count >= self.n_in_row:\n",
    "                        return True, player\n",
    "                else:\n",
    "                    count = 0\n",
    "                    break\n",
    "            # y=-x\n",
    "            for j in range(self.n_in_row):\n",
    "                if y-j < 0 or x+j >=self.width:\n",
    "                    count = 0\n",
    "                    break\n",
    "                if self.board[self.xy2move(x+j ,y-j)] == Stone:\n",
    "                    count += 1\n",
    "                    if count >= self.n_in_row:\n",
    "                        return True, player\n",
    "                else:\n",
    "                    count = 0\n",
    "                    break\n",
    "        return False, -1\n",
    "\n",
    "    def load_game(self, history):\n",
    "        # self.restart()\n",
    "        for m in history:\n",
    "            winner = self.play(m)\n",
    "            print(f'move: {m}')\n",
    "            self.visual()\n",
    "            if winner == self.Continue:\n",
    "                continue\n",
    "            elif winner == self.Black:\n",
    "                print('black has win')\n",
    "            else:\n",
    "                print('white has win')\n",
    "\n",
    "    def restart(self):\n",
    "         self.board = [self.Empty for i in range(self.width*self.height)]\n",
    "         self.cur_player = self.Black\n",
    "         self.history = []\n",
    "\n",
    "    def feature(self):\n",
    "        \"\"\"\n",
    "        四张棋盘  黑方落子 白方落子 上一回合落子 当前落子方\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        fea = torch.zeros((4, self.width,self.height))\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                move = self.xy2move(x, y)\n",
    "                if self.board[move] == self.Blackstone:\n",
    "                    fea[0][x][y] = self.Black\n",
    "                elif self.board[move] == self.Whitestone:\n",
    "                    fea[1][x][y] = self.White\n",
    "        m = self.history[-1]\n",
    "        x, y = self.move2xy(m)\n",
    "        fea[2][x][y] = self.board[m]\n",
    "        fea[3] = self.cur_player\n",
    "        return fea"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# game = Game()\n",
    "# game.load_game([1, 8, 2, 9, 3, 10, 4])\n",
    "# fea = game.feature()\n",
    "# fea"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,conf:Configuration):\n",
    "        super(Net, self).__init__()\n",
    "        self.board_width = conf.width\n",
    "        self.board_height = conf.height\n",
    "        # 通用层 common layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # 行动策略层 action policy layers\n",
    "        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n",
    "        self.act_fc1 = nn.Linear(4*self.board_width*self.board_height,\n",
    "                                 self.board_width*self.board_height)\n",
    "        # 状态值层 state value layers\n",
    "        self.val_conv1 = nn.Conv2d(128, 2, kernel_size=1)\n",
    "        self.val_fc1 = nn.Linear(2*self.board_width*self.board_height, 64)\n",
    "        self.val_fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, feature):\n",
    "         # 通用层 common layers\n",
    "        x = F.relu(self.conv1(feature))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # 行动策略层 action policy layers\n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = x_act.view(-1, 4*self.board_width*self.board_height)\n",
    "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
    "        # 状态值层 state value layers\n",
    "        x_val = F.relu(self.val_conv1(x))\n",
    "        x_val = x_val.view(-1, 2*self.board_width*self.board_height)\n",
    "        x_val = F.relu(self.val_fc1(x_val))\n",
    "        x_val = F.tanh(self.val_fc2(x_val))\n",
    "        # 输出行动可能性 和 终局的预期状态值\n",
    "        return x_act, x_val\n",
    "# conf = Configuration()\n",
    "# net = Net(conf)\n",
    "# input = torch.zeros((2,4,conf.width,conf.height))\n",
    "# acts,q = net(input)\n",
    "# print(acts.shape)\n",
    "# print(q.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, parent=None, pi=1.0, cput=1.0):\n",
    "        self.parent = parent\n",
    "        self.P = pi\n",
    "        self.children = {}  # children[act]=child\n",
    "        self.Q = 0.0\n",
    "        self.N = 0.0\n",
    "        self.cput = cput\n",
    "\n",
    "    def update(self, v):\n",
    "        self.Q = (self.Q * self.N + v) / (self.N + 1)\n",
    "        self.N = self.N + 1\n",
    "\n",
    "    def uct(self):\n",
    "        return self.Q + self.cput * self.P * np.sqrt(self.parent.N ) / (self.N + 1)\n",
    "\n",
    "    def backup(self, v):\n",
    "        if self.parent is not None:\n",
    "            self.parent.backup(-v)\n",
    "        self.update(v)\n",
    "\n",
    "    def select(self):\n",
    "        \"\"\"\n",
    "        return child's act and node with max uct\n",
    "        \"\"\"\n",
    "        return max(self.children.items(),key=lambda act_node:act_node[1].uct())\n",
    "\n",
    "    def expand(self, act_pi):\n",
    "        \"\"\"\n",
    "        expand childs with (act,pi)\n",
    "        \"\"\"\n",
    "        for (act, pi) in act_pi:\n",
    "            if act not in self.children:\n",
    "                self.children[act] = Node(self, pi, self.cput)\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def __str__(self):\n",
    "        s = f\"\"\n",
    "        for (act, child) in self.children.items():\n",
    "            s += f\"{act}:{child.uct}\"\n",
    "        return s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game:Game, cput, model, conf:Configuration,search_time=5):\n",
    "        self.cput = cput\n",
    "        self.game = game\n",
    "        self.root = Node(parent=None, pi=1.0, cput=cput)\n",
    "        self.model=None\n",
    "        if model is not None:\n",
    "            self.model = model.to(conf.device)\n",
    "        self.search_time = search_time\n",
    "        self.conf = conf\n",
    "\n",
    "    def reset(self):\n",
    "        self.root = Node(parent=None, pi=1.0, cput=self.cput)\n",
    "\n",
    "    def policy(self,game, feature):\n",
    "\n",
    "        if self.model is None:\n",
    "            prob = [1/(game.height*game.width) for i in range(game.height*game.width)]\n",
    "            for m in game.history:\n",
    "                prob[m] = 0.0\n",
    "            act_prob = [(move,prob[move]) for move in range(self.conf.width*self.conf.height)]\n",
    "            q = 0\n",
    "        else:\n",
    "            feature = feature.to(self.conf.device)\n",
    "            p,q = self.model(feature)\n",
    "            prob = np.exp(p.data.cpu().flatten().numpy()).tolist()\n",
    "            for m in game.history:\n",
    "                prob[m] = 0.0\n",
    "            act_prob = [(move,prob[move]) for move in range(self.conf.width*self.conf.height)]\n",
    "            q = q.data\n",
    "        return act_prob,q\n",
    "\n",
    "    def search(self):\n",
    "        start = time.time()\n",
    "        while time.time()-start <= self.search_time:\n",
    "            self.simulate()\n",
    "\n",
    "    def simulate(self):\n",
    "        node = self.root\n",
    "        game = copy.deepcopy(self.game)\n",
    "        # select 到 leaf\n",
    "        flag= False\n",
    "        winner = -1\n",
    "        while True:\n",
    "            if node.is_leaf():\n",
    "                break\n",
    "            act,node = node.select()\n",
    "            winner = game.play(act)\n",
    "            flag = True if winner != game.Continue else False\n",
    "\n",
    "        # evaluate\n",
    "        action_probs, q_value = self.policy(game,game.feature())\n",
    "\n",
    "        # expand\n",
    "        if not flag:\n",
    "            node.expand(action_probs)\n",
    "            if self.model is None:\n",
    "                return\n",
    "        else:\n",
    "            if winner == -1:\n",
    "                q_value = 0.0\n",
    "            elif winner==game.cur_player:\n",
    "                q_value = 1.0\n",
    "            else:\n",
    "                q_value = -1.0\n",
    "\n",
    "        # update\n",
    "        node.backup(-q_value)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        probs = np.exp(x - np.max(x))\n",
    "        probs /= np.sum(probs)\n",
    "        return probs\n",
    "\n",
    "    def get_move_probs(self,temp=1e-3):\n",
    "        act_visits = [(act, node.N) for act, node in self.root.children.items()]\n",
    "        acts, visits = zip(*act_visits)\n",
    "        act_probs = self.softmax(1.0/temp * np.log(np.array(visits) + 1e-10))\n",
    "        return acts, act_probs\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self,conf:Configuration):\n",
    "        self.conf = conf\n",
    "        self.device = conf.device\n",
    "        self.learn_rate = conf.learn_rate # 基准学习率\n",
    "        self.lr_multiplier = conf.lr_multiplier # 基于KL自动调整学习倍速\n",
    "        self.temp = conf.temp # 温度参数\n",
    "        self.search_time = conf.search_time # 每下一步棋，搜索时间\n",
    "        self.c_puct = conf.c_puct # exploitation和exploration之间的折中系数\n",
    "        self.buffer_size = conf.buffer_size\n",
    "        self.batch_size = conf.batch_size # mini-batch size for training\n",
    "        self.data_buffer =conf.data_buffer #使用 deque 创建一个双端队列\n",
    "\n",
    "        self.play_batch_size = conf.play_batch_size\n",
    "        self.epochs = conf.epochs  # num of train_steps for each update\n",
    "        self.kl_targ = conf.kl_targ # 早停检查\n",
    "        self.check_freq = conf.check_freq # 每50次检查一次，策略价值网络是否更新\n",
    "\n",
    "        self.game_batch_epoch= conf.game_batch_epoch # 训练多少个epoch\n",
    "        self.best_win_ratio = conf.best_win_ratio# 当前最佳胜率，用他来判断是否有更好的模型\n",
    "        # 弱AI（纯MCTS）模拟步数，用于给训练的策略AI提供对手\n",
    "        self.pure_mcts_playout_num = conf.pure_mcts_playout_num\n",
    "        self.init_model = conf.init_model\n",
    "        self.net = Net(conf)\n",
    "        if self.init_model is not None:\n",
    "            net_parm = torch.load(self.init_model)\n",
    "            self.net.load_state_dict(net_parm)\n",
    "\n",
    "\n",
    "        self.l2_const = 1e-4  # L2正则项系数\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), weight_decay=self.l2_const)\n",
    "\n",
    "    # 通过旋转和翻转增加数据集, play_data: [(state, mcts_prob, winner_z), ..., ...]\n",
    "    def get_equi_data(self, play_data):\n",
    "        extend_data = []\n",
    "        for state, mcts_porb, winner in play_data:\n",
    "            # 在4个方向上进行expand，每个方向都进行旋转，水平翻转\n",
    "            for i in [1, 2, 3, 4]:\n",
    "                # 逆时针旋转\n",
    "                equi_state = np.array([np.rot90(s, i) for s in state])\n",
    "                equi_mcts_prob = np.rot90(np.flipud(mcts_porb.reshape(self.conf.width, self.conf.height)), i)\n",
    "                extend_data.append((equi_state, np.flipud(equi_mcts_prob).flatten(), winner))\n",
    "                # 水平翻转\n",
    "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
    "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
    "                extend_data.append((equi_state, np.flipud(equi_mcts_prob).flatten(), winner))\n",
    "        return extend_data\n",
    "\n",
    "    def collect_selfplay_data(self, n_games=1):\n",
    "        game = Game()\n",
    "        mcts = MCTS(game,1.0,self.net,self.conf,search_time=self.search_time)\n",
    "        playdata =[]\n",
    "        for i in range(n_games):\n",
    "             # 记录该局对应的数据：states, mcts_probs, current_players\n",
    "             states, mcts_probs, current_players = [], [], []\n",
    "             game.restart()\n",
    "             mcts.reset()\n",
    "             while True:\n",
    "                mcts.search()\n",
    "                acts, act_probs = mcts.get_move_probs(temp=self.temp)\n",
    "                # history\n",
    "                mcts_probs.append(act_probs)\n",
    "                states.append(game.feature().cpu().numpy().tolist())\n",
    "                current_players.append(game.cur_player)\n",
    "\n",
    "                move = acts[act_probs.index(max(act_probs))]\n",
    "                winner = game.play(move)\n",
    "                game.visual()\n",
    "                end = True  if winner !=game.Continue or len(game.history) == game.width*game.height else False\n",
    "\n",
    "                if end:\n",
    "                    # end\n",
    "                    winners_z = np.zeros(len(current_players))\n",
    "                    winners_z[np.array(current_players) == winner] = 1.0\n",
    "                    winners_z[np.array(current_players) != winner] = -1.0\n",
    "\n",
    "                    if winner == game.Black:\n",
    "                        print('black has win')\n",
    "                    else:\n",
    "                        print('white has win')\n",
    "\n",
    "                    playdata.extend(list(zip(states, mcts_probs, current_players)))\n",
    "        # 保存下了多少步\n",
    "        self.episode_len = len(playdata)\n",
    "        playdata = self.get_equi_data(playdata)\n",
    "        self.data_buffer.extend(playdata)\n",
    "\n",
    "    def train_step(self,state_batch,\n",
    "                    mcts_probs_batch,\n",
    "                    winner_batch,\n",
    "                    lr):\n",
    "        state_batch= torch.tensor(state_batch).to(self.device)\n",
    "        mcts_probs_batch= torch.tensor(mcts_probs_batch).to(self.device)\n",
    "        winner_batch = torch.tensor(winner_batch).to(self.device)\n",
    "        # 清空模型中参数的梯度，即梯度置为0\n",
    "        self.net.zero_grad()\n",
    "        # 设置学习率\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # 前向传播\n",
    "        log_act_probs, value = self.net(state_batch)\n",
    "        # 定义 loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
    "        value_loss = F.mse_loss(value.view(-1), winner_batch)\n",
    "        policy_loss = -torch.mean(torch.sum(mcts_probs_batch*log_act_probs, 1))\n",
    "        loss = value_loss + policy_loss\n",
    "        # 反向传播，优化参数\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # 计算Policy信息熵\n",
    "        entropy = -torch.mean(torch.sum(torch.exp(log_act_probs) * log_act_probs, 1))\n",
    "        # 返回loss和entropy\n",
    "        return loss.item(), entropy.item()\n",
    "     # 更新策略网络\n",
    "    def policy_update(self):\n",
    "        mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
    "        state_batch = [data[0] for data in mini_batch]\n",
    "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
    "        winner_batch = [data[2] for data in mini_batch]\n",
    "        # 保存更新前的old_probs, old_v\n",
    "        old_probs, old_v = self.net(state_batch)\n",
    "        for i in range(self.epochs):\n",
    "            # 每次训练，调整参数，返回loss和entropy\n",
    "            loss, entropy = self.net.train_step(\n",
    "                    state_batch,\n",
    "                    mcts_probs_batch,\n",
    "                    winner_batch,\n",
    "                    self.learn_rate*self.lr_multiplier)\n",
    "            # 输入状态，得到行动的可能性和状态值，按照batch进行输入\n",
    "            new_probs, new_v = self.net(state_batch)\n",
    "            # 计算更新前后两次的loss差\n",
    "            kl = np.mean(np.sum(old_probs * (\n",
    "                    np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
    "                    axis=1)\n",
    "            )\n",
    "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
    "                break\n",
    "        # 动态调整学习倍率 lr_multiplier\n",
    "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
    "            self.lr_multiplier /= 1.5\n",
    "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
    "            self.lr_multiplier *= 1.5\n",
    "\n",
    "        explained_var_old = (1 -\n",
    "                             np.var(np.array(winner_batch) - old_v.flatten()) /\n",
    "                             np.var(np.array(winner_batch)))\n",
    "        explained_var_new = (1 -\n",
    "                             np.var(np.array(winner_batch) - new_v.flatten()) /\n",
    "                             np.var(np.array(winner_batch)))\n",
    "        print((\"kl:{:.5f},\"\n",
    "               \"lr_multiplier:{:.3f},\"\n",
    "               \"loss:{},\"\n",
    "               \"entropy:{},\"\n",
    "               \"explained_var_old:{:.3f},\"\n",
    "               \"explained_var_new:{:.3f}\"\n",
    "               ).format(kl,\n",
    "                        self.lr_multiplier,\n",
    "                        loss,\n",
    "                        entropy,\n",
    "                        explained_var_old,\n",
    "                        explained_var_new))\n",
    "        return loss, entropy\n",
    "\n",
    "    # 用于评估训练网络的质量，评估一共10场play，返回比赛胜率（赢1分、输0分、平0.5分）\n",
    "    def policy_evaluate(self, n_games=10):\n",
    "        game = Game()\n",
    "        current_mcts_player = MCTS(game,self.c_puct,self.net,self.conf,search_time=3)\n",
    "        pure_mcts_player = MCTS(game,self.c_puct,None,self.conf,search_time=3)\n",
    "        win_cnt = defaultdict(int)\n",
    "        for i in range(n_games):\n",
    "            # AI和弱AI（纯MCTS）对弈，不需要可视化 is_shown=0，双方轮流职黑 start_player=i % 2\n",
    "            winner = self.game.start_play(current_mcts_player, pure_mcts_player, start_player=i % 2, is_shown=0)\n",
    "            win_cnt[winner] += 1\n",
    "        # 计算胜率，平手计为0.5分\n",
    "        win_ratio = 1.0*(win_cnt[1] + 0.5*win_cnt[-1]) / n_games\n",
    "        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(\n",
    "                self.pure_mcts_playout_num,\n",
    "                win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
    "        return win_ratio\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "             # 训练game_batch_num次，每个batch比赛play_batch_size场\n",
    "            for i in range(self.game_batch_epoch):\n",
    "                # 收集自我对弈数据\n",
    "                self.collect_selfplay_data(self.play_batch_size)\n",
    "                print(\"batch i:{}, episode_len:{}\".format(i+1, self.episode_len))\n",
    "                # train\n",
    "                if len(self.data_buffer) > self.batch_size:\n",
    "                    loss, entropy = self.policy_update()\n",
    "                # 判断当前模型的表现，保存最优模型\n",
    "                if (i+1) % self.check_freq == 0: #50次檢查一次\n",
    "                    print(\"current self-play batch: {}\".format(i+1))\n",
    "                    win_ratio = self.policy_evaluate()\n",
    "                    # 保存当前策略\n",
    "                    self.net.save_model('./current_policy.model')\n",
    "                    if win_ratio > self.best_win_ratio:\n",
    "                        print(\"发现新的最优策略，进行策略更新\")\n",
    "                        self.best_win_ratio = win_ratio\n",
    "                        # 更新最优策略\n",
    "                        self.net.save_model('./best_policy.model')\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\n\\rquit')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tsize mismatch for act_fc1.weight: copying a param with shape torch.Size([49, 196]) from checkpoint, the shape in current model is torch.Size([35, 140]).\n\tsize mismatch for act_fc1.bias: copying a param with shape torch.Size([49]) from checkpoint, the shape in current model is torch.Size([35]).\n\tsize mismatch for val_fc1.weight: copying a param with shape torch.Size([64, 98]) from checkpoint, the shape in current model is torch.Size([64, 70]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conf\u001B[38;5;241m.\u001B[39minit_model \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m      6\u001B[0m     net_parm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(conf\u001B[38;5;241m.\u001B[39minit_model)\n\u001B[1;32m----> 7\u001B[0m     \u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet_parm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m mcts \u001B[38;5;241m=\u001B[39m MCTS(game,\u001B[38;5;241m1.0\u001B[39m,net,conf,search_time\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\environment\\anconda5.3.0\\envs\\pytorch118\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict)\u001B[0m\n\u001B[0;32m   2036\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[0;32m   2037\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2038\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[0;32m   2040\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2041\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2042\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[0;32m   2043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Net:\n\tsize mismatch for act_fc1.weight: copying a param with shape torch.Size([49, 196]) from checkpoint, the shape in current model is torch.Size([35, 140]).\n\tsize mismatch for act_fc1.bias: copying a param with shape torch.Size([49]) from checkpoint, the shape in current model is torch.Size([35]).\n\tsize mismatch for val_fc1.weight: copying a param with shape torch.Size([64, 98]) from checkpoint, the shape in current model is torch.Size([64, 70])."
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
